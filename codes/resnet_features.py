# -*- coding: utf-8 -*-
"""Resnet_features.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b_iDh1OTo_0omwh5wNFLd9AWb_7wb5ik

# **Resnet**
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.preprocessing import image
from scipy.spatial.distance import cdist

import requests
import io
from scipy.spatial.distance import cdist
import numpy as np

# Updated train_path to download the raw file content
train_path = f'https://github.com/psarath22/CS771_M2/raw/832c7eced7cd8dfcc800a7cfd8b772fefbf89560/dataset/part_one_dataset/train_data/1_train_data.tar.pth'
# Download the file from the URL
response = requests.get(train_path, stream=True)
response.raise_for_status()  # Raise an exception for bad responses

# Load the data from the downloaded content using io.BytesIO
train_data = torch.load(io.BytesIO(response.content))

print(train_data.keys())
print(train_data['data'].shape)
print(train_data['targets'].shape)

# Get the first 15 images and labels from train_data
images = train_data['data'][:15]
labels = train_data['targets'][:15]

# Set up a 3x5 plot
fig, axes = plt.subplots(3, 5, figsize=(10, 3))

for i, ax in enumerate(axes.flat):
    ax.imshow(images[i])
    ax.set_title(f"Label: {labels[i]}")
    ax.axis('off')  # Hide the axes for a cleaner look

plt.tight_layout()
plt.show()

# Initialize feature extractor model
model = ResNet50(weights='imagenet', include_top=False, pooling='avg')

# Feature extraction function
def process_and_extract_features(data, model):
    extracted_features = []
    for X in data:
        X_resized = np.array([image.img_to_array(image.array_to_img(img, scale=False).resize((180,180))) for img in X])
        X_preprocessed = preprocess_input(X_resized)
        features = model.predict(X_preprocessed, batch_size=32)
        extracted_features.append(features)
    return np.vstack(extracted_features)

class LwPClassifier:
    def __init__(self, metric='euclidean'):
        self.metric = metric
        self.class_prototypes = {}

    def fit(self, X, y, weights=None, alpha=0.01, max_iter=100, tol=1e-4):
        """Compute prototypes (mean vectors) for each class, with optional weighting and regularization."""
        X = X.reshape(X.shape[0], -1)  # Flatten the feature dimension
        y = np.array(y, dtype=int)  # Ensure y is a 1D array of integers

        # Initialize prototypes using weighted average if weights are provided, else use simple average
        if weights is None:
            prototypes = np.array([
                np.mean(X[y == cls], axis=0)  # Use simple average if no weights
                for cls in np.unique(y)
            ])
        else:
            # Iterative refinement (like K-means) for pseudo-labels (for datasets 2 to 10)
            prototypes = np.array([
                np.average(X[y == cls], axis=0, weights=weights[y == cls])  # Initial prototypes with weights
                for cls in np.unique(y)
            ])

            for iteration in range(max_iter):
                print(f"Iteration {iteration+1}/{max_iter}")

                # Step 1: Assign each sample to the closest prototype
                distances = cdist(X, prototypes, metric=self.metric)
                closest_prototype_idx = distances.argmin(axis=1)

                # Step 2: Update prototypes based on the assigned closest samples (weighted average)
                new_prototypes = np.array([
                    np.average(X[closest_prototype_idx == cls], axis=0, weights=weights[closest_prototype_idx == cls] if weights is not None else None)
                    if np.any(closest_prototype_idx == cls) else prototypes[cls]
                    for cls in np.unique(y)
                ])

                # Step 3: Check for convergence (if the change in prototypes is smaller than the tolerance)
                if np.all(np.abs(new_prototypes - prototypes) < tol):
                    print("Convergence reached.")
                    break

                prototypes = new_prototypes

        # Update the class prototypes after training
        self.class_prototypes = {cls: prototypes[cls] for cls in np.unique(y)}

    def predict(self, X):
        """Predict class for each sample in X."""
        X = X.reshape(X.shape[0], -1)  # Flatten the feature dimension
        prototypes = np.array([self.class_prototypes[cls] for cls in self.class_prototypes])
        distances = cdist(X, prototypes, metric=self.metric)
        closest_prototype_idx = distances.argmin(axis=1)
        return np.array([cls for cls in self.class_prototypes])[closest_prototype_idx]

# Initialize accuracy matrix and define number of models
num_models = 20
accuracy_matrix = np.zeros((num_models, num_models))

# Helper functions to get data and labels in appropriate format
def get_data_fn1(data):
    return data['data']

def get_data_fn2(data):
    return data['data'], data['targets']

# Pre-extract features for all training and evaluation datasets
train_data_extracted = {}
eval_data_extracted = {}
labels_train = {}
labels_eval = {}

# Pre-process train datasets
for i in range(1, 21):
    if i <= 10:
        train_path = f'https://github.com/psarath22/CS771_M2/raw/832c7eced7cd8dfcc800a7cfd8b772fefbf89560/dataset/part_one_dataset/train_data/{i}_train_data.tar.pth'  # Use 'raw' instead of 'blob'
    else:
        train_path = f'https://github.com/psarath22/CS771_M2/raw/832c7eced7cd8dfcc800a7cfd8b772fefbf89560/dataset/part_two_dataset/train_data/{i-10}_train_data.tar.pth'  # Use 'raw' instead of 'blob'


    if i==1:
        response = requests.get(train_path, stream=True)
        train_data = torch.load(io.BytesIO(response.content))
        X_train,Y_train = get_data_fn2(train_data)
        X_train_features = process_and_extract_features([X_train], model)
        labels_train[i] = Y_train
    else:
        response = requests.get(train_path, stream=True)
        train_data = torch.load(io.BytesIO(response.content))
        X_train = get_data_fn1(train_data)
        X_train_features = process_and_extract_features([X_train], model)

    train_data_extracted[i] = X_train_features

# Pre-process+ eval datasets
for i in range(1, 21):
    if i <= 10:
        eval_path = f'https://github.com/psarath22/CS771_M2/raw/832c7eced7cd8dfcc800a7cfd8b772fefbf89560/dataset/part_one_dataset/eval_data/{i}_eval_data.tar.pth'
    else:
        eval_path = f'https://github.com/psarath22/CS771_M2/raw/832c7eced7cd8dfcc800a7cfd8b772fefbf89560/dataset/part_two_dataset/eval_data/{i-10}_eval_data.tar.pth'

    response = requests.get(eval_path, stream=True)
    response.raise_for_status()  # Raise an exception for bad responses

    eval_data = torch.load(io.BytesIO(response.content))
    X_valid, Y_valid = get_data_fn2(eval_data)
    X_valid_features = process_and_extract_features([X_valid], model)

    eval_data_extracted[i] = X_valid_features
    labels_eval[i] = Y_valid

print(train_data_extracted[1].shape)

# Initialize classifier and train on the first dataset
classifier = LwPClassifier(metric='euclidean')
classifier.fit(train_data_extracted[1], labels_train[1])

# Evaluate on the first dataset
valid_predictions_1 = classifier.predict(eval_data_extracted[1])
valid_accuracy_1 = accuracy_score(labels_eval[1], valid_predictions_1)
accuracy_matrix[0, 0] = valid_accuracy_1
print(f"Model f1: Validation Accuracy on D1 = {valid_accuracy_1:.2f}")

# Train and evaluate models on subsequent datasets
for i in range(2, 21):
    X_train = train_data_extracted[i]

    # Generate pseudo-labels and update prototypes
    pseudo_labels = classifier.predict(X_train)
    X_combined = X_train
    Y_combined = pseudo_labels
    classifier.fit(X_combined, Y_combined)  # Update prototypes with combined data

    # Evaluate the current model on the current and all previous validation sets
    for j in range(1, i + 1):
        X_valid_j = eval_data_extracted[j]
        Y_valid_j = labels_eval[j]

        valid_predictions = classifier.predict(X_valid_j)
        valid_accuracy = accuracy_score(Y_valid_j, valid_predictions)
        accuracy_matrix[i - 1, j - 1] = valid_accuracy

# Print accuracies row-wise
for i in range(num_models):
    print(f"Accuracies for model f{i+1}: {accuracy_matrix[i, :i+1]}")

# Plotting the accuracy matrix as a heatmap
plt.figure(figsize=(20, 10))
heatmap = plt.matshow(accuracy_matrix, cmap='YlGnBu', fignum=1, vmin=0, vmax=1.0)  # Set color range from 0.6 to 1.0

# Add color bar for reference with accuracy label
plt.colorbar(heatmap, label='Accuracy')

# Add annotations to each cell with the accuracy values
for i in range(num_models):
    for j in range(i + 1):  # Only show lower triangle values
        plt.text(j, i, f"{accuracy_matrix[i, j]:.2f}", ha='center', va='center', color="black")

# Adjust tick labels to start from 1 instead of 0
plt.xticks(ticks=range(num_models), labels=range(1, num_models + 1))
plt.yticks(ticks=range(num_models), labels=range(1, num_models + 1))

# Set axis labels
plt.xlabel("Dataset Index")
plt.ylabel("Model Index")
plt.title("Accuracy Matrix (Intensity by Accuracy)")

# Show the plot
plt.show()

